\documentclass[12pt]{article}
%Gumm{\color{blue}i}|065|=)
\usepackage{amsmath, amsfonts, amssymb}
\usepackage[margin=0.5in]{geometry}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{hyperref}

\usepackage{fontspec}
\usepackage{xcolor}

\newcommand{\off}[1]{}
\DeclareMathSizes{20}{30}{20}{18}
\usepackage{tikz}

%\setmainfont[Color=brown]{Linux Libertine}


\title{Tune-Up: First Moment Method}
\date{}
\begin{document}

\sffamily

\maketitle

{\fontsize{16pt}{16pt}\selectfont 

\noindent Probabilistic Method is a technique invented in the 20th century.  We don't have time to check all the possible answers, we are trying to merely check that an object of a given type exists within our co-hort and maybe of an unusual or rare type, given the information we know. \\ \\
When we say random, we could ask ``How often does event $[X]$ occur?" The mathematical name for the change of the information we get from learning something is be expressed with \textbf{conditional probability}.   \\ \\
\textbf{Ex} Prove (e.g. using pigeonhole principle)

\begin{itemize}

\item $\mathbf{P}\big[X \geq \mathbb{E}[X]\big] > 0$ sometimes the unpredictible ``chance" thing is greater than we expected.

\item $\mathbf{P}\big[X \leq \mathbb{E}[X]\big] > 0$ sometimes the unpredictible ``chance" thing is less than what we expected. 

\end{itemize}
How are we able to extrapolate such meaningful arguments from such ``duh!" starting points? Probability is interesting since we never get to know what the correct probability measure should have been.  Probability theory doesn't even make sense if the event only happens \textit{once}. \\ \\
\begin{tikzpicture}
\draw[fill=black, opacity=0.25, color=black] (0,0) circle (1);
\draw[fill=black, opacity=0.25, color=black] (1,0) circle (1);
\end{tikzpicture}\\
\textit{Proof} In general, $\mathbf{P}[A \cup B] \leq \mathbf{P}[A] + \mathbf{P}[B]$. Let's test our particular case:  \\ \\
$1 = \mathbf{P}[(X \leq \mathbb{E}[X]) \cup (X \geq \mathbb{E}[X])] \leq
\mathbf{P}[X \leq \mathbb{E}[X]] + \mathbf{P}[ X \geq \mathbb{E}[X]]  $ \\ \\
Uh...\\ \\
$1 \geq a \geq 0$ and $1 \geq b \geq 0$ and $a + b \geq 1$ we'd like to conclude $a > 0$ and $b > 0$. \\ \\
Well... \\ \\
If $b \geq 1 - a = 0$ then $a = 1$. In our case,  $\mathbf{P}[X \leq \mathbb{E}[X]] = 0$ and $\mathbf{P}[X \geq \mathbb{E}[X]] = 1$. \\ \\
That looks fine... \\ \\
$\mathbf{P}[X > \mathbb{E}[X]] = 1$ and yet $\mathbf{P}[X \geq \mathbb{E}[X]] = 1$. I'm guessing $\mathbf{P}[X = \mathbf{E}[X]] = 0$. \\ \\
Why can't $\mathbf{P}[X < \mathbb{E}[X]] = 0$? \\ \\
Our value is \textit{never} less than what we expected.  Then maybe we should set our expectations higher? \\ \\
We always have $X > \mathbb{E}[X]$ then
$$ \mathbb{E}[X] \leq \sum_{X \leq \mathbb{E}[X]} x\,\mathbb{P}[x] +  \sum_{X \geq \mathbb{E}[X]} x\,\mathbb{P}[x] = 0  +  \sum_{X >  \mathbb{E}[X]} x\,\mathbb{P}[x]  $$
if we always have $X > \mathbb{E}[X]$ then
$$ \mathbb{E}[X] \leq \sum_{X \leq \mathbb{E}[X]} x\,\mathbb{P}[x] +  \sum_{X \geq \mathbb{E}[X]} x\,\mathbb{P}[x] = \sum_{X <  \mathbb{E}[X]} x\,\mathbb{P}[x] + 0
 < \mathbb{E}[X] \; \mathbb{P}[X < \mathbb{E}[X]]$$
if we have that $\mathbb{E}[X] > 0$ then we concllude that $\mathbf{P}[X < \mathbb{E}[X]] > 1$.  Unfortunately we do not have this assumption. \hfill $\varnothing$\\ \\
\textit{Proof} How about we have that $\mathbb{E}[X - \mathbb{E}[X]] = 0$, this is called ``centering".
$$ 0 = \mathbb{E}[X - \mathbb{E}[X]] = \sum_{X \leq \mathbb{E}[X]} (x - \mathbb{E}[X])\;\mathbf{P}[x]
+ \sum_{X \geq \mathbb{E}[X]} (x - \mathbb{E}[X])\;\mathbf{P}[x]$$  
By definition of our idea of ``probability" $\mathbf{P}[x] \geq 0$ and yet the total of these numbers is zero.  Therefore some term is positive and some term is negative, we have $x > \mathbb{E}[X]$ and $x < \mathbb{E}[X]$ in some case. \\ \\
Cleainup?  Maybe using conditional probablity:
\begin{eqnarray*} 0  &=& \mathbb{E}[X - \mathbb{E}[X]] \\ 
&=& \mathbb{E}\Big[X - \mathbb{E}[X] | X \leq \mathbb{E}[X]\Big] \mathbf{P}( X \leq \mathbb{E}[X]) \\
&+& \mathbb{E}\Big[X - \mathbb{E}[X] | X \geq \mathbb{E}[X]\Big] \mathbf{P}( X \geq \mathbb{E}[X]) \\
&=& a\mathbf{P}( X \leq \mathbb{E}[X]) + b \mathbf{P}( X \geq \mathbb{E}[X])
\end{eqnarray*}
where $a \leq 0$ and $b \geq 0$.  Yet:
$$\mathbf{P}( X \leq \mathbb{E}[X]) +  \mathbf{P}( X \geq \mathbb{E}[X]) \geq 1 $$
$$ 0 = a\mathbf{P}( X \leq \mathbb{E}[X]) + b \mathbf{P}( X \geq \mathbb{E}[X])
\geq a\mathbf{P}( X \leq \mathbb{E}[X]) + b (1 -  \mathbf{P}( X \leq \mathbb{E}[X]))$$
Then maybe if we arrange more carefully:
$$ 0 \geq (a - b)\mathbf{P}(X \leq \mathbb{E}[X]) + b $$
Let's try one more:
$$ \mathbf{P}(X \leq \mathbb{E}[X]) \geq \frac{b}{b-a} $$
Maybe easier:
$$ (-a)\mathbf{P}( X \leq \mathbb{E}[X]) = b \mathbf{P}( X \geq \mathbb{E}[X])$$
Let's rule out $a = 0$ and $b = 0$.  If $a = 0$, either
\begin{itemize}
\item $\mathbf{P}(X \leq \mathbf{E}[X]) = 0$ and $P (X \geq \mathbf{E}[X]) = 1$
\item $b = 0 $
\end{itemize}
$$  \mathbb{E}\Big[X  | X \leq \mathbb{E}[X]\Big] -  \mathbb{E}[X]  = \mathbb{E}\Big[X - \mathbb{E}[X] | X \leq \mathbb{E}[X]\Big] = 0 $$
Looks like we need to consider the event $\mathbb{P}[ X = \mathbb{E}[X]] > 0$ that there is probability concentrated at one point .

Another great option is \dots our world doesn't stop because we find one or two or even a hundred contradictions.  Our logic keeps going our culture keeps going and we keep existing.  How do we humans reason in such murky and contradictoary environment? \\ \\

\newpage

\noindent \textbf{01/22} \\ \\
\textit{Hints}:
\begin{itemize}
\item $\mathbb{E}[X | X > 0] > 0$
\item $\mathbf{P}(X > 0) + \mathbf{P}(X = 0) + \mathbf{P}(X > 0) = 1$
\item Let $\mathbb{E}[X] = 0$ then $0 = \mathbb{E}[X | X > 0]\, \mathbf{P}(X > 0)
+ 0 + \mathbb{E}[X | X < 0]\, \mathbf{P}(X < 0 )$
\item $ 1 > 0$
\end{itemize}
We are showing that:
\begin{itemize}
\item $\mathbf{P}(X > 0) + \mathbf{P}(X = 0) > 0$ or
\item $\mathbf{P}(X < 0) < 1$
\end{itemize}
In fact we have a sort of ultimatum:
\begin{itemize}
\item $\mathbf{P}(X > 0) = 0$ and $\mathbf{P}(X < 0) = 0$ (and then $\mathbb{E}[X] = 0$)
\item $\mathbf{P}(X > 0) > 0$ and $\mathbf{P}(X < 0) > 0$ (and then $\mathbf{P}(X \geq 0) > 0$)
\end{itemize}
So ...  \\ \\
We are relying heavily on that $X$ and $\mathbb{E}[X]$ are \textit{numbers} (and their properties) and that $X$ is taking values in $\mathbb{R}$.  As math major and a scientist there is no other way.  \\ \\
There's a recursive nature about this problem, turning into itself which sort of clumsy.  There's asymmetry between these kinds of relations (``$>$" vs. ``$\geq$") and (``$>$" vs. ``$<$"). \\ \\
There's a question about \textbf{mathematical proof} and use randomly putting together formulas until we reach a result.  The end proof is a sort of a ``tree" as well. \\ \\
... \\ \\
Why go though such a headache?  I've seen first-moment menthod use non-trivially before.  It's likely also a description of how we actually \textit{solve} problems in general.  There's also \textit{second-moment} method. Pick one at random, let the process run and we find some statistic to show that our event occurs with positive chance. 
\newpage

\noindent \textbf{Answer Key} \textit{Markov's Inequality}.  Let $X$ be a non-negative random variable.  Then for any positive real $\lambda > 0$:
$$ \mathbf{P}(X \geq \lambda) \leq \frac{\mathbb{E}(X)}{\lambda} $$
Proof: we have the trivial inequality.  $X \geq \lambda \mathbf{I}(X \geq \lambda)$.
\begin{itemize}
\item if $X < \lambda$ the inequality says $X \geq 0$
\item if $X \geq \lambda$ it says $X \geq \lambda$
\end{itemize}
Indeed if we take expectation of both sides, by \textbf{linearity of expectation} we get the result.  
$$ \mathbb{E}[X] \geq \lambda \mathbb{P}(X \geq \lambda) $$
Maybe if we drew a picture, the shape of this random variable could be more clear:\\
\begin{tikzpicture}
\draw[->] (-2,0)--(2,0);
\draw[->] (0,-2)--(0,2);
\node at (0.5,2) {$X$};
\node at (2.5,0.5) {$\lambda$};
\draw[dashed] (-2,1)--(2,1);
\draw[line width=2] (-2,0)--(1,0);
\draw[line width=2] (1,1)--(2,1);
\draw[black!10!white] (1,0)--(1,1);
\draw[fill=white] (1,0) circle (0.1);
\draw[fill=white] (1,1) circle (0.1);
\end{tikzpicture} \\ \\
There's something self-referential about this pure mathematical framework.  These number systems are limits of what we measure in ``real life".  The notion of \textbf{5} or ``five" as  ``the thing after $4$" ... numbers are recursively defined.

\vfill

\begin{thebibliography}{}

\item Terence Tao, Van Vu.  \textbf{Additive Combinatorics} (Cambridge Advanced Studies in Mathematics \#105) Cambridge University Press, 2006.

\end{thebibliography}

\end{document}