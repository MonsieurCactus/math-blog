<!DOCTYPE html>
<html>

<head>
<title>John's Paranoid Ranting</title>

<style type="text/css">
#content {width: 60%; float: left; text-align: justify; padding-left: 5%;}

blockquote{ background-color: #EDA749; padding: 1%}


</style>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>
<!-- <script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script> -->

<script type="text/javascript"
  src="MathJax-2.3-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


</head>

<xmp theme="spruce" style="display:none;">

<style type="text/css">

blockquote a { color: #BA5215;}

</style>

# Cauchy Schwarz vs Pigeonhole

### [1]() - What is Cauchy-Schwartz?

Math textbooks agree the Cauchy-Schwartz inequality is very useful.  One version of it says:

$$ (a_1^2 + a_2^2 + a_3^2)^{1/2}(b_1^2 + b_2^2 + b_3^2)^{1/2} \geq a_1 b_1 + a_2 b_2 + a_3 b_3$$

This inequality can be written very succintly using vectors in 3D space.

$$ || \vec{a}||  \; ||\vec{b} || \geq a \cdot b $$

In fact, it didn't really atter how many dimensions we were dealing with, as long as we have
an *inner product space*.  Usually the inner product is just the dot product of vectors, but we 
can have many other inner products:

$$ \int_a^b f(x)^2 dx \int_a^b g(x) dx \geq \int_a^b f(x) g(x) dx $$

This is the Cauchy Schwartz inequality, where the "vectors" are functions $f:[a,b] \to \mathbb{R}$.  Identifying
the correct choice of vectors or inner product is an art, and they appear randomly and unannounced in math papers.

---

Since there is no index **All Examples of Cauchy-Schwartz** inequality, I found myself taking random 
books hoping they had an example, and reading through all the proofs to see if it was mentioned.
The result was unilluminating, since I was so fried from scanning the pages to actually read an absorb them.
I found several examples and it was always the same... the use of Cauchy-Schwartz was clear, just as in the book.
What was *not* clear, was how the two observables were constructed or how they moved the proof forward.

I noticed that books on **measure theory** tend to make clever **partitions** and estimate each partition separately.
Cauchy-Schwartz inequality necessarily involves inner product spaces and the most natural context for that
is Fourier series and generalizations.

I don't know many *instances* or *patterns* to when the Cauchy Schwartz inequality is used... I don't even know how to ask such a question.  Maybe "Favorte example of Cauchy Schwartz inequality?" or ask meta.MathOverflow to refine this question.

---

Let's **prove** the Cauchy Schwartz inequality in two ways.

> \#1 **Law of Cosines**
> 
> For any triangle, the law of cosines says $ (a+b)^2 = a^2 + b^2 + 2 |a| \, |b| \cos \theta$ and yet the inner product formula says
$$|a+b|^2 = (a+b)\cdot (a+b) = |a|^2 + |b|^2 - 2(a\cdot b)$$  This is a positive number
and we oppose the law of cosines and the definition of inner product $$2|a|\, |b| \cos \theta = (a+b)^2 - ( a^2 + b^2) = 2 |a \cdot b| $$
If you are afraid the Law of Cosines only works in two dimensons, just remember any Euclidean triangle has to lie in some 2D plane!

Geometric inequalities?

> \#2 **Distance from point to line**
>
> Which point on the line $\vec{a} + t \vec{b}$ is closes to the origin?  Let's compute the norm
> $$ |\vec{a} + t \vec{b}|^2 = |a|^2 + 2 t (\vec{a} \cdot \vec{b}) + t^2 \vec{b}\cdot \vec{b}$$
> This is always a positive number, and it's a quadratic function.  So the discriminant is positive.
> That discriminant is $4 \times \big( |\vec{a}| |\vec{b}| - (\vec{a} \cdot \vec{b})\big) \geq 0$. 

Yet another geometric interpretation.  What's going on here?  For reasons we can't explain this is a fundamental inequality, but we observe
the Law of Cosines is playing a huge role.

**What are the inequalities good for, outside of inequalities?** I can't give you an answer to that.  Even relating one branch of math to another I have trouble asking, what is this stuff good for?

Geometrically, using the Cauchy-Schwartz inequality always signifies looking for correlations.  Taking two vectors and ignoring the angle between them.  If we knew the angle $\theta$ between two vectors, maybe we could get better estimates?

> Let $z_1, \dots, z_n$ be vectors with $|z - 1| &lt; r$.  Then 
> $$ |z_1 + \dots + z_n| \, \left|\frac{1}{z_1} + \dots \frac{1}{z_n} \right|  \geq n^2(1 -r^2)$$
> 
> [China Math Olympiad](http://math.stackexchange.com/questions/1075250/how-prove-this-complex-inequality-with-same-as-2014-china-cmo-cauchy-schwarz-i) 2014 

Does Cauchy-Schwartz inequality give us enough information to solve this problem?  The poser of the question first thinks about using Lagrange's identity:

$$ (\mathbf{a} \cdot \mathbf{a})(\mathbf{b} \cdot \mathbf{b}) - (\mathbf{a} \cdot \mathbf{b})^2 
= (\mathbf{a} \wedge \mathbf{a})\cdot (\mathbf{a} \wedge \mathbf{a}) $$

If you don't like the wedge product, there is a cross product for vectors in **3** or **7** dimensions:

$$ |\mathbf{a} \times \mathbf{b}|^2 = \left| \begin{array}{cc}  \mathbf{a} \cdot \mathbf{a}&\mathbf{b} \cdot \mathbf{a} \\\\ 
\mathbf{a} \cdot \mathbf{b}& \mathbf{b} \cdot \mathbf{b} \end{array} \right| 
=  (\mathbf{a} \cdot \mathbf{a})(\mathbf{b} \cdot \mathbf{b}) - (\mathbf{a} \cdot \mathbf{b})^2 $$

Here $\mathbf{a}, \mathbf{b} \in \mathbb{R}^3$ or $\mathbf{R}^7$.  On the one hand, these can generalize involving the **quaternions**
or [**octonions**](https://en.wikipedia.org/wiki/Seven-dimensional_cross_product).  On the other hand, we can generlize this to the [Cauchy-Binet](https://en.wikipedia.org/wiki/Binet%E2%80%93Cauchy_identity) identity, involving determinants of matrices.

If these were real numbers, the Cauchy-Schwartz inequality returns a different lower bound:

$$ (|z_1| + \dots + |z_n|) \left( \frac{1}{|z_1|} + \dots + \frac{1}{|z_n|} \right) \geq n^2 $$

We don't consider any of the cancellation between the $z$ or the $\frac{1}{z}$.  The correct observable involves creating a
vector from the real parts:

$$ \left(\mathrm{Re}(z_1), \dots, \mathrm{Re}(z_n) \right) \text{ and }
 \left(\mathrm{Re}(\tfrac{1}{z_1}), \dots, \mathrm{Re}(\tfrac{1}{z_n}) \right) $$

This is a rather strange triangle to consider with vertices $z, \frac{1}{z}, z + \frac{1}{z}$ for $z \in \mathbb{C}$ with the extra information that $z$ is close to $1$: $|z - 1| &lt; r$.

### [2]() - What is the Pigeonhole Principle

The Pigeonhole principle says given $m+1$ objects in $m$ slots, two of them must have been put into the same slot.  This statement is written so abstractly, it's hard to decide when to use or what conclusions to gain.

Example \#1 comes from number theory, and we might call it **Dirichlet's Pigeonhole Principle**.

> Consider the $N+2$ numbers, $0<  \{ \alpha \}, \{2 \alpha \} ,\dots,  \{ n \alpha \}  &lt; 1$.  These are our "pigeons" and the $N+1$ "slots" are intervals of the form $[ \tfrac{k}{N+1}, \tfrac{k+1}{N+1}]$.  Therefore, $$|| n \alpha || <  \frac{1}{N+1}$$
for some $0 \leq n \leq N$. 

Dirichlet's approximation theorem can be generalized to vectors, to numbers fields and even beyond that, to **homogenous spaces**.  It is believe the Dirichlet unit theorem.  Searching on MathOverflow seems to give a version of the paper by Artin and Whaples.

I want to know why there aren't too many applications of pigeonhole principle to *enumerative combinatorics* or any combinatorics at all.  That subject seems to be focused on clever bijections, generating functions and inclusion-exclusion and other methods.  Instead, these inequalities are used in *extremal combinatorics* where the sets are hard to describe or imagine.

### [3]() - Poincare Recurrence

In light of some cryptic remarks on MathOverflow, we try to understand better why the Cauchy Schwartz inequality and the Pigeonhole Princple are two sides of the same coin.  Initially they look completely different.  How can

$$ (x^2 + y^2 + z^2)^{1/2}(a^2 + b^2 + c^2)^{1/2} \geq ax + by + cz$$

be the same as painting 4 boxes with three colors, two of them being the same? 
&#91;
[1](http://mathoverflow.net/questions/582/cauchy-schwarz-and-pigeonhole),
[2](http://tcs.nju.edu.cn/wiki/index.php/%E7%BB%84%E5%90%88%E6%95%B0%E5%AD%A6\_%28Fall\_2011%29/Extremal\_graph\_theory), [3](https://terrytao.wordpress.com/2014/06/05/when-is-correlation-transitive/) &#93;.

[**Poincare's Recurrence Theorem**](https://terrytao.wordpress.com/2008/01/30/254a-lecture-8-the-mean-ergodic-theorem/) states:

> Let $(X, \mathcal{X}, \mu, T)$ be a measure preserving dynamical system, and let $E \subseteq X$ be a subset of positive measure.
Then $$ \limsup_{n \to \infty} \mu(E \cap T^n E) \geq \mu(E)^2 $$
In paricular $E \cap T^nE$ has positive measure for infinitely many positive $n$.

A dynamical system describes any process over time that can be described in a predictible fashion.
*Entropy* of that dynamical system might have to do with our inability to predict the initial condition,
and then we have to gauge how far down the evolution are system has changed.

For example $T: x \mapsto 2x \mod 1$ multiplies a number by $2$ and takes the part after the decimal.  
If we are wrong with our initial condition, it will only take a few steps before are system state is unpredictible.
How unpredictible?  We will learn 1 bit of information every step the system evolves.

> Any time we have a measure we can write out the step function supported on $E$
> $$   \int 1_E \\, d\mu =   \mu(E)  $$  
> 
> This is like saying "if  I had a nickel for every time I steped on $E$, blah blah".  Since $T$ is measure preserving, we can instead 
get a nickel for every time we step on $T^n E$ for any integer $n > 0$.
> $$   \int 1_{T^n E} \\, d\mu =   \mu(E)  $$  
>
> We can add up all these inequalities to get another true equality.
> $$ \int \sum\_{n=1}^N 1\_{T^n E} \, d\mu  = N \mu(E) $$


We don't know very much about the function $f = \sum 1_{T^n E}$ except that it's integral over all space is $N \mu(E)$.
If we steped on both $E \cap T(E)$ then $f \geq 2$. 

> The Cauchy-Schwartz inequality says, instead of squaring the outside, we can square the inside and get a larger number:
> $$   \int \sum\_{n=1}^N \left(1\_{T^n E}\right)^2 \, d\mu   \geq  \left( \int \sum\_{n=1}^N 1\_{T^n E} \, d\mu \right)^2 =  N^2 \mu(E)^2  $$ 
> 

---

*Out in the open wisdom calls aloud, she raises her voice in the public square;* Proverbs 1:20

</xmp>


<!-- <script src="http://strapdownjs.com/v/0.2/strapdown.js"></script> -->

<script src="strapdown/v/0.2/strapdown.js"></script>

</html>
