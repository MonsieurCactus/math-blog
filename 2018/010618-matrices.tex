\documentclass[12pt]{article}
%Gummi|065|=)
\usepackage{amsmath, amsfonts, amssymb}
\usepackage[margin=0.5in]{geometry}
\usepackage{xcolor}
\usepackage{graphicx}

%\usepackage{pifont}
\usepackage{amsmath}

\newcommand{\off}[1]{}
\DeclareMathSizes{20}{30}{20}{18}

\newcommand{\two }{\sqrt[3]{2}}
\newcommand{\four}{\sqrt[3]{4}}
\newcommand{\red}{\begin{tikz}[scale=0.25]
\draw[fill=red, color=red] (0,0)--(1,0)--(1,1)--(0,1)--cycle;\end{tikz}}
\newcommand{\blue}{\begin{tikz}[scale=0.25]
\draw[fill=blue, color=blue] (0,0)--(1,0)--(1,1)--(0,1)--cycle;\end{tikz}}
\newcommand{\green}{\begin{tikz}[scale=0.25]
\draw[fill=green, color=green] (0,0)--(1,0)--(1,1)--(0,1)--cycle;\end{tikz}}

\newcommand{\sq}[3]{\draw[#3] (#1,#2)--(#1+1,#2)--(#1+1,#2+1)--(#1,#2+1)--cycle;}

\usepackage{tikz}

\newcommand{\susy}{{\bf Q}}
\newcommand{\RV}{{\text{R}_\text{V}}}

\title{Scratchwork: Gaussian Integral}
\date{}
\begin{document}

%\fontfamily{qag}\selectfont \fontsize{12.5}{15}\selectfont

\sffamily

\maketitle

\noindent Matrix identities are infinite. \\ \\
\textbf{Ex.} In 2003, Fyodorov and Strahov found an identity for $N \times N$ Hermitian Gaussian random matrix, $H$ and let $D(z) = \det (z - H)$ be the characteristic polynomial.  Let $u_1, \dots, u_d \in \mathbb{C} \backslash \mathbb{R}$ and $v_1, \dots v_d \in \mathbb{C}$. 
$$ 
\left\langle 
\frac{D(v_1) \dots D(v_d)}{D(u_1) \dots D(u_d)}\right\rangle = 
\det \left( \frac{1}{u_i - v_j} \right)^{-1} \cdot
\det \left\langle \frac{1}{u_i - v_j} \frac{D(v_j)}{D(u_i)} \right\rangle
$$
This is a comparison of two $d \times d$ matrices, which are themselves expectation of another random process.  \\ \\
This is my blog, we can work out the numerous proofs that have emerged between the years 2000-2005.  This offers a springboard into various things. \\ \\
\textbf{Ex.} The latest moviation I can think of for studying the Gaussian comes from Mochizuki:
$$
\textbf{Gaussians} \to \dots \to \text{Degrees of Arithemtic Line Bundles} \to \dots \to \text{Teichmuller Theory} \to \dots \to \textbf{IUT} $$
I always wondered what makes the Gaussian special.  Whenenever we want to ``solve" something we do a sequences of manipulations (preserving all information) or approximations (losing some info) and deform our old problem into an example we're more familiar with. \\ \\
{ \color{black!50!white} \textbf{Theory}}
All problem solving consists of these types of approximations and deformations.  The shape that Mochizuki offers us is just the one:
$$
\left(
\int_{\mathbb{R}} e^{-x^2/2} \, dx \right) \left( \int_{\mathbb{R}} e^{-y^2/2} \, dy  \right) 
= \int_{\mathbb{R} \times \mathbb{R}}
e^{-(x^2 + y^2)} \, dx dy 
= \left(\int_0^\infty r \, dr e^{r^2/2} \right) \left( \int_{[0, 2\pi]} d\theta\right) = 2\pi $$
and we extracted several ideas from this solutions including but not limited to:
\begin{itemize}
\item change of variables
\item differential maps
\item jacobians
\item fibrations
\item seperation of variables
\item probability
\end{itemize}
and Mochizuki says there's even more\dots there's almost all of number theory.  As a babe step, he feels the theta function is a kind of ``Gaussian":
$$ \theta(z) = \sum_{n \in \mathbb{Z}} e^{2\pi i \, n^2 z}  $$
There are many many theta functions and these form a mess called the theory of ``Abelian varieties".  The theory of Gaussians looks to me like the following decomposition:
$$ \mathbb{R}^2 = \mathbb{R} \times \mathbb{R} 
= \Big( \mathbb{R}_{\geq 0} \times S^1 \Big)  \cup \{ 0 \} $$
is this something I just made up or is it functorial?  It's not a common way to think about the Gaussian.  Within the mathematical community (a small group of people) there are a wide range of opinions, and it will still be unconventional. \\ \\
Mochizuki -- who is quite controversial already -- is telling us to hit there.  I'd take his word for it. Then, we become like him. \\ \\
\textbf{Ex.} A few more examples from Knot theory.
$$
\mathcal{Z}_{\text{CS}}(S^3; q)
= \frac{1}{N!} \int \prod_{i=1}^N
\frac{dx_i}{2\pi} e^{\frac{1}{2g}x_i^2}\prod_{i < j}^N \left( 2 \sinh \frac{x_i - x_j}{2} \right)^2 $$
This is Gaussian integral.  Go!
\vfill

\begin{thebibliography}{}

\item Gerald V. Dunne, Mithat \"{U}nsal  \textbf{Resurgence and Trans-series in Quantum Field Theory: The $\mathbb{C}P^{N-1}$ Model} \texttt{arXiv:1210.2423}

\item Maxim Kontsevich \textbf{Exponential Integral} \texttt{https://www.youtube.com/watch?v=tM25X6AI5dY}
 
\end{thebibliography}

\begin{thebibliography}{}

\item Alexei Borodin, Grigori Olshanski, Eugene Strahov \textbf{Giambelli compatible point processes}
\texttt{arXiv:0505021}

\item Bertrand Eynard, Taro Kimura. \textbf{Towards U(N|M) knot invariant from ABJM theory
}
\texttt{arXiv:1408.0010}

\end{thebibliography}

\newpage

\noindent Where were we? The Fyodorov-Strahov integral as many many proofs.  We can try ourselves and compare with other arguments.  The formulas have two parameters, we are integrating over $N \times N$ Hermitian matrices and there are $d$ points, regardless of the size of matrix.  \\ \\
\textbf{1} To the best of my knowledge the $d = 1$ case is degenerate:
$$ \left\langle \frac{D(v)}{D(u)} \right\rangle 
=  \left(\frac{1}{u-v} \right)^{-1} \cdot 
\left\langle \frac{1}{u-v} \frac{D(v)}{D(u)} \right\rangle $$
without evaluating anything at all, there is a cancellation.  We recall that $\langle \cdot \rangle$ is a linear operator.  It looks like this:
$$ \langle X \rangle = c^{-1} \langle cX \rangle  = (c^{-1}c) \langle X \rangle $$
This argument looks rather trivial, as long as we stay in the sandbox where it's guaranteed to work. The miracle, is that it will continue to do work for us in other circumstances. \\\\ However, we did not evaluate $\langle X\rangle$. We haven't even explained that $D(\cdot)$ is the characteristic polynomial. The Wick formula was like that, we evaluated moments in terms of other moments.  \\ \\
\textbf{2} As we move forward, we will quickly run out of variables.  
$$
\left\langle 
\frac{D(v_1)D(v_2)}{D(u_1) D(u_2)}
\right\rangle 
= \left| 
\begin{array}{cc}
\frac{1}{u_1 - v_1} & \frac{1}{u_1 - v_2} \\ \\
\frac{1}{u_2 - v_1} & \frac{1}{v_1 - v_2}  \end{array}\right|^{-1}  \;\;
\left| 
\begin{array}{cc}
\left\langle \frac{1}{u_1 - v_1} \frac{D(v_1)}{D(u_1)} \right \rangle & 
\left\langle \frac{1}{u_1 - v_2} \frac{D(v_2)}{D(u_1)} \right \rangle \\ \\
\left\langle \frac{1}{u_2 - v_1} \frac{D(v_1)}{D(u_2)} \right \rangle & 
\left\langle \frac{1}{u_2 - v_2} \frac{D(v_2)}{D(u_1)} \right \rangle  \end{array}\right|$$
I have not seen any of the standard\footnote{``standard" means maybe 20 people in the world have done it and maybe a few people more could do it if they had time.  And they don't have time.} proofs yet.
  Mutually alien copies means we should do the intgral twice and multiply them:
\begin{eqnarray*}
\left\langle 
\frac{D(v_1)D(v_2)}{D(u_1) D(u_2)}
\right\rangle^2
 &=&
\left\langle 
\frac{D(v_1)D(v_2)}{D(u_1) D(u_2)}
\right\rangle 
\left\langle 
\frac{D(v_1)D(v_2)}{D(u_1) D(u_2)}
\right\rangle \\ \\
&=& \left\langle \frac{\det (v_1 - H_1)\det (v_2 - H_1)}{
\det (u_1 - H_1)\det (u_2 - H_1)} \right\rangle \cdot
\left\langle \frac{\det (v_1 - H_2)\det (v_2 - H_2)}{
\det (u_1 - H_2)\det (u_2 - H_2)} \right\rangle \\ \\
&=& 
\left\langle \frac{\det (v_1 - H_1)\det (v_2 - H_1)}{
\det (u_1 - H_1)\det (u_2 - H_1)} \cdot \frac{\det (v_1 - H_2)\det (v_2 - H_2)}{
\det (u_1 - H_2)\det (u_2 - H_2)} \right\rangle 
 \end{eqnarray*} 
Next, it depends on how badly we want to solve the problem.  Or do we wish to explore.  The resources available on $2 \times 2$ matrices, vastly outnumber the results we actually use.  E.g. $\det(\cdot)$ is a homomorphism:
$$ \det(AB) = (\det A )(\det B) \hspace{0.25in}\text{and}\hspace{0.25in} \det(A\, A^{-1}) = \det(I_{N \times N}) = 1$$
The complex numbers $\mathbb{C}$ are a very idealized context compared to measurements we actually use.  There are now 8 variables floating around. \\ \\
What would be the outcome of this result?  The RHS is the product of a $2 \times 2$ matrix and a $2\times 2 $ matrix of $2 \times 2$ matrices.  
$$ (V \wedge V) \oplus \Big((V \otimes V) \wedge (V \otimes V)\Big) $$
Perhaps I should write $\wedge$ instead of $\otimes$ in different places.  The first factor has $2^2 = 4$ entries, the second factor has $2^4 = 16$ entries.   And maybe we need to invent a new symbol. 

\newpage

\noindent $\to$ \textbf{1} what does this look like if I try to evaluate the integral from $1$.  We are looking for the general shape: 
\begin{eqnarray*}
\left\langle 
\frac{\det (u - H)}{\det (v - H)} \right\rangle^2 
&=& \left\langle 
\frac{\det (u - H_1)}{\det (v - H_1)} \right\rangle
\left\langle 
\frac{\det (u - H_2)}{\det (v - H_2)} \right\rangle \\\\
&=& \int dH_1 \, dH_2 \; e^{\mathrm{tr}(H_1^2) + \mathrm{tr}(H_2^2)} \;
\frac{\det (u - H_1)}{\det (v - H_1)} \cdot
\frac{\det (u - H_2)}{\det (v - H_2)} 
\end{eqnarray*}
Do I remember any linear algebra at all?   There is a formula for the charactristic polynomial of $2 \times 2$ matrix:
$$ \det (x - A) = x^2 - x\,\mathrm{tr}(A) + \det A $$
It's even worthwhile to write down the characteristic polynomial of $3 \times 3$ matrix:
$$ x^3 - \mathrm{tr}(A) \, x^2 + \frac{1}{2} \left[  \mathrm{tr}^2(A) - \mathrm{tr}(A^2)\right]\,x
- \frac{1}{6} \left[
\mathrm{tr}^3(A) + 2\, \mathrm{tr}(A^3) - 3 \,\mathrm{tr}(A) \mathrm{tr}(A^2)
 \right] =0$$
Fortunately, we can imagine that if we diagonalize $H_1$ and $H_2$ then maybe we can diagonalize $u - H_1$ and $v - H_2$.  We obtain two copies of the unitary group, and another Gaussian integral:
$$
\int dH  \; e^{\mathrm{tr}(H^2) } \;
\frac{\det (u - H)}{\det (v - H)} 
= \int d\lambda_1\, d\lambda_2 \; e^{-(\lambda_1^2 + \lambda_2^2)} \; \left[ \;
(\lambda_1 - \lambda_2)^2 \; \frac{(u - \lambda_1)(u - \lambda_2)}
{(v - \lambda_1)(v - \lambda_2)} \;\right]$$
This is not working out the way I wanted it to.
\end{document}