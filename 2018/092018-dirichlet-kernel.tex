\documentclass[12pt]{article}
%Gumm{\color{blue}i}|065|=)
\usepackage{amsmath, amsfonts, amssymb}
\usepackage[margin=0.5in]{geometry}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{amsmath}

\newcommand{\off}[1]{}
\DeclareMathSizes{20}{30}{20}{18}
\usepackage{tikz}


\title{Scratchwork: Basic Theorems of Lebesgue Integration}
\date{}
\begin{document}

\sffamily

\maketitle

\noindent \textbf{9/20} Why do we need Riemann integration or even Lebesgue integration.  Here's an integral formula:
$$ \int_0^1 x^2 \, dx = \frac{1}{3}$$
In calculus class we just memorize a rule: $\int: x^n \to \frac{1}{n+1}x^{n+1}$ and apply the formula.  Here's one more:
$$ \int_0^{x} \cos x \, dx = \sin x $$
So there is a second rule $\int: \cos x \mapsto \sin x$. Once we buy into these one or two or a dozen rules, we can corner ourselves very quickly.  \textbf{Ex}: Show that 
$$ \int_0^{2\pi} f(x) \, \big( \sin Nx \big)^2 \, dx \to \pi f(x) $$
The formula looks right, we have that $0 < \sin^2 x < 1$ and it oscillates fairly evenly so the average should be $\frac{1}{2}$. \\ \\
Riemann integration was already a formality, Lebesgue integration was an even bigger formality. Here's a common-sense looking theorem that requires Lebesgue integration. \\ \\
\textbf{Problem}: Exchange integration and summation.  Let $f_k$ be a sequence of $L^1$ integrable functions such that $\sum_1^\infty |f_j| < \infty$.  Then $\sum f_k$ converges (almost everywhere) to a function in $L^1$ and 
$$ \int \sum_{i=1}^\infty f_k (x) \, dx = \sum_{i=1}^\infty \int f_k (x) \, dx $$
If the function sequences look forbidding, let $f_n(x) = a_n \, \sin n x$ with $0 \leq x \leq 1$. We want to know if
$$ \int \sum_{i=1}^\infty a_n \, \sin (2\pi n x) \, dx = \sum_{i=1}^\infty \frac{a_n}{n} \, \cos(2\pi n x)  $$
There's even an easy choice $a_n = \frac{1}{\pi \, k}$. We obtain the sawtooth wave:
$$  \sum_{m=1}^\infty \frac{\sin 2\pi m x}{\pi k } = \frac{1}{2} - \{ x\} $$
It's the only one we ever study.  Despite it's usefulness in music, I'm mostly concered about it's arithmetic and geometric properties.  This ``=" sign is really shaky too.  At $x = \pi$ 
$$  \sum_{m=1}^\infty \frac{0 \pm \epsilon }{\pi k } \approx 0 \pm \left(\frac{1}{2} - \epsilon \right) $$
Fortunately for us, nature (and certainly Mathemtical Physics) will offer us examples of natural processes that require Lebesgue theory to understand.

\newpage

\noindent \textbf{Problem}  When do limits and integrals converge?
$$ \int f \, d\mu= \lim_{n \to \infty} \int f_n  \, d\mu $$
If we look on the previous page of the Lebesgue theory textbook, we get conditions when this common-sense formula might work:
\begin{itemize}
\item $f_n$ is a sequence of $L^1$ functins (Lebesgue integrable)\footnote{Did we ever compute a Lebesgue integral in our lives?  So why we bother talking about Lebesgue integrable} with $f_n \to f$ almost everywhere.
\item There's a (non-negative) function $g \geq 0$ such that $|f_n| \leq g$ (almost everywhere).
\end{itemize}
Certainly the authors of these textbooks have lost their minds, and they are writing such a textbook for their health. \\ \\
\textbf{Thm} with these conditions, the limit $f \in L^1$ and $\int f = \lim_{n \to \infty} \int f_n$. \\ \\
If we ever want this common-sense result to whole we need to more results from Lebesgue integration theory:
\begin{itemize}
\item Dominated Convergence Theorem
\item Fatou's Lemma
\item Monotone Convergence Theorem
\end{itemize}
Lebesgue measure is impossible to construct.  Half of probability (e.g. the Law of Large Numbers) is showing that a measure either converges to Lebesgue measure or to a point (or to a Gaussian centered at that point). \\ \\
\textbf{Example} Here's a set that requires measure theory to even try to estimate the size of.  Let $\alpha = \sqrt[4]{2}$ and consider the set $\{ (m,n) : 0 < m^2 + \sqrt[4]{2} \, n^2 < X \} $.  Here is a measure we could study:
$$ \mu(x) = \sum 1_{m^2 + \sqrt[4]{2}\,
n^2}(x) $$
There were two possible ways to write this set.  Any major differences?
\begin{itemize}
\item $\{ m^2 + \sqrt[4]{2}\,n^2 : 0 \leq m,n \leq N \}$
\item $\{ m^2 + \sqrt[4]{2}\,n^2 : 0 \leq m \leq M \; 0 \leq n \leq N \}$
\item $\{ (m,n) : 0 < m^2 + \sqrt[4]{2} \, n^2 < X \} $
\end{itemize}
The last set is a collection of pairs of integers (coordinates) and the first two sets are collections of real numbers. \\ \\
Here's a function we might study with Lebesgue theory:
$$ f(x) = \big| \{ (m,n) : 0 < m^2 + \sqrt[4]{2} \, n^2 < X \} \big| - \frac{\pi}{4\sqrt[4]{2}} X \approx 0$$
we've used nothing but familiar household objects and the equations describe a natural thing.

\vfill

\begin{thebibliography}{}

\item Gerald B. Folland \textbf{Real Analysis: Modern Techniques and their Applications}.  Wiley, 1999.

\item Valentin Blomer, Jean Bourgain, Maksym Radziwi\l\l, Zeev Rudnick \textbf{Small gaps in the spectrum of the rectangular billiard} \texttt{arXiv:1604.02413}

\end{thebibliography}

\newpage

\noindent We've been fiddling with the Prime Number Theorem.  It starts to get really confusing.  We are trying to estimate:
$$ A(x) =  \sum_{n < x} \Lambda(n) \quad\text{ so that }\quad \alpha(s) = \int_1^\infty x^{-s} \, d A(x) = \sum_{n=0}^\infty \Lambda(n) \, n^{-s} $$
This is a Mellin transform.  We are using the Riemann Stieltjes integral, since there are lots of jumps.  $d[1_{x < 0}] = \delta_0(x)$ e.g. using theory of distributions.  The numbers are primes:
$$ \Lambda(n) = \left\{ \begin{array}{rl} \log p & x = p^k \\ \\ 0 &  \text{otherwise} \end{array} \right. $$
Originally we have an exponential series insted of a Dirichlet series and we are using a Laplace transform:
\[ \alpha(s) = \int_0^\infty e^{-xs} \, da(x) = \sum_{n=0}^\infty \Lambda(n) \, e^{-ns} \text{ with }a(x) = A(e^x) = \sum_{n < e^x} \Lambda(n) \tag{$\ast$} \]
The theoretical version deals with this version in the middle.  We could even have power series, the most normal-looking:
$$ a(x) = \sum_{n=0}^\infty \Lambda(n) \, x^n $$
The reason why the Van Mangoldt function was so important is because it was the GCD of the first $n$ numbers:
$$ e^{\sum_{n < x} \Lambda(n) } = \text{LCM}(\{1,2,\dots, x\})$$
and $\Lambda$ is the M\"{o}bius transform of the logarithm.  We can turn one into the other by summing over the divisors:
$$ \log n = \sum_{m|n} \Lambda(n) \text{ or }\log = \Lambda * 1 \quad\text{ and }\quad \Lambda(n) = \sum_{m|n} \log(m) \text{ or } \Lambda = \log * 1 $$
Now we read the proof.  We need a ``copy" of the exponential function $e^x$ (we're even afraid to call it that... $E(x)$) that is compactly supported in Foureir space.  The exponent function is definitely not.
$$ E(x) = \left\{ \begin{array}{cc} e^x & x < 0 \\ 0 & x > 0 \end{array} \right. $$
This function has a discontinuity at zero the right-limit $E(0^-) = 1$ and left-limit $E(0^+)=0$.  This function has a Fourier transform:
$$ \hat{E}(t) = \int_{-\infty}^0 e^{x - tx} \, dx = \frac{1}{1-t}$$
The reason that we need a compactly-supported cousin of the expoenntial function the pole at $t=1$. 
\begin{itemize}
\item $\displaystyle \Delta_T(x) = T \left( \frac{\sin \pi T x}{\pi T x} \right)^2$ \;\;Fejer \;\;\, Kernel over $\mathbb{R}$.  
\item $\displaystyle J_T(x) = \frac{3T}{4} \left( \frac{\sin \pi \frac{T}{2} x}{\pi \frac{T}{2} x}\right)^4$ Jackson Kernel over $\mathbb{R}$
\end{itemize}
One way to think of all these function is that these things all approxiate boxes of area $1$:
$$ \Delta_T(x) \approx J_T(x) \approx  1_{|x| < \frac{1}{T}} \times T $$
These Kernels arise from doubts as to whether the Fourier series of a function converges to itself.  No engineer in his right mind questions that, but we have a counter-example on the previous page.
\end{document}